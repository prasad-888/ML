{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3de5d9b6-1b68-4f91-94a6-e2840510b8db",
   "metadata": {},
   "source": [
    "## How KNN Works as a Regressor\n",
    "\n",
    "In classification, KNN determines the class of a point by majority voting among its k-nearest neighbors. In regression, the approach is similar, but instead of voting, it averages the target values of the k-nearest neighbors.\n",
    "\n",
    "1. Calculate distances: Compute the distance (e.g., Euclidean distance) between the query point and all points in the training data.\n",
    "2. Identify k-nearest neighbors: Select the k points that are closest to the query point.\n",
    "3. Aggregate outputs: Take the average of the target values of these neighbors to predict the output.\n",
    "Example\n",
    "\n",
    "Suppose you want to predict the salary of an employee with 3.5 years of experience. Using KNN with k=3, the algorithm finds the three closest data points:\n",
    "\n",
    "*  Neighbor 1: 3 years of experience → $45,000 \n",
    "*  Neighbor 2: 4 years of experience → $50,000\n",
    "*  Neighbor 3: 3.8 years of experience → $48,000\n",
    "\n",
    "Prediction=(45,000+50,000+48,000)/3=47,666.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99f5aaef-0a13-48e3-8d22-e4908e46aefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "X = [[1], [2], [3], [4], [5]] # experience in years\n",
    "y= [30, 40, 45, 50, 55]  # Salary in $1000\n",
    "\n",
    "#Initialize and fit the KNN regressor\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=3)\n",
    "knn_regressor.fit(X, y)\n",
    "\n",
    "\n",
    "# Predict salary for 3.5 years of experience\n",
    "print(knn_regressor.predict([[3.5]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c09ae5-7c8f-43b5-998d-23c08cf4a810",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM) Regression\n",
    "Working:\n",
    "\n",
    "SVM for regression, known as Support Vector Regression (SVR), predicts a continuous output by finding a hyperplane that fits the data with a margin of tolerance.\n",
    "\n",
    "1. Define margin \\text{<span class=\"katex\"><span class=\"katex-mathml\">\\epsilon}: Identify a margin of tolerance where predictions within this margin are not penalized.\n",
    "2. Find optimal hyperplane: Use support vectors (key data points) to construct the hyperplane that minimizes prediction error while staying within the margin.\n",
    "3. Kernel trick: Extend to non-linear data using kernel functions like RBF or polynomial.\n",
    "\n",
    "For example, if a point lies within the \\text{<span class=\"katex\"><span class=\"katex-mathml\">\\epsilon}-tube, no penalty is applied. Points outside the margin contribute to the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70595064-7984-4b6f-8538-f1ab188d745c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.35181503]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "X = [[1], [2], [3], [4], [5]]\n",
    "y = [2.1, 2.9, 3.7, 4.5, 5.3]\n",
    "\n",
    "# Initialize and fit SVR\n",
    "svr = SVR(kernel='rbf', C=1, epsilon=0.1)\n",
    "svr.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "print(svr.predict([[2.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f166cb-6eed-4a7f-968f-1d91a23694f8",
   "metadata": {},
   "source": [
    "## Decision Tree Regression\n",
    "Working:\n",
    "\n",
    "Decision Tree Regression splits the data into smaller subsets based on feature values to minimize the variance in each subset.\n",
    "\n",
    "1. Choose the best split: Evaluate all possible splits and choose the one that reduces the variance in target values the most.\n",
    "2. Create branches: Split the data into branches based on the selected feature value.\n",
    "3. Repeat recursively: Continue splitting until a stopping condition is met (e.g., maximum depth or minimum samples per leaf).\n",
    "\n",
    "For instance, if splitting on experience at 2.5 years minimizes variance, that split becomes a branch, and predictions within that branch are averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ce5d164-07df-4954-b724-d6ce579b823d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.9]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "X = [[1], [2], [3], [4], [5]]\n",
    "y = [2.1, 2.9, 3.7, 4.5, 5.3]\n",
    "\n",
    "#Initialize and fit the regressor \n",
    "dt_regressor = DecisionTreeRegressor(max_depth=3)\n",
    "dt_regressor.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "print(dt_regressor.predict([[2.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e83100-6f46-4676-95dd-9b0b086325e0",
   "metadata": {},
   "source": [
    "# Random Forest Regression\n",
    "Working:\n",
    "\n",
    "Random Forest Regression combines predictions from multiple decision trees to improve accuracy and reduce overfitting.\n",
    "\n",
    "1. Build multiple trees: Construct decision trees on different subsets of data and features.\n",
    "2. Aggregate predictions: Take the average of all tree predictions to get the final output.\n",
    "For example, if three trees predict values [3.0,3.2,3.5], the output is:\n",
    "\n",
    "Prediction=(3.0+3.2+3.5)/3=3.2333...≈3.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37e62abc-ff7d-4554-8165-16c35c8c5451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.06]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X = [[1], [2], [3], [4], [5]]\n",
    "y = [2.1, 2.9, 3.7, 4.5, 5.3]\n",
    "\n",
    "rf_regressor = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "rf_regressor.fit(X, y)\n",
    "\n",
    "print(rf_regressor.predict([[2.5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef1fecd-b51a-4d35-89cd-ca7468cc5007",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
