{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b3668d6-0681-459e-ad9e-645cd02437e1",
   "metadata": {},
   "source": [
    "### Vectorizing\n",
    "Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect __numerical feature vectors with a fixed size__ rather than the __raw text documents with variable length.__\n",
    "\n",
    "In order to address this issue, early researchers came up with some methods to extract numerical features from text content. All these methods have the following steps in common:\n",
    "\n",
    "1. **Tokenizing** strings, for instance, by using white-spaces and punctuation as token separators. And then giving an integer-id for each possible token.\n",
    "\n",
    "2. __Counting__ the occurrences of tokens in each string/sentence/document.\n",
    "\n",
    "3. **Normalizing and Weighting** with diminishing importance tokens that occur in the majority of samples / documents.\n",
    "\n",
    "A set of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.\n",
    "\n",
    "In NLP, document can be a string, english sentence or complete word file. Also, __corpus__ is nothing but collection of document. Research community likes to use a lot of technical, domain-specific jargon. Your job is to not let these words scare you!\n",
    "\n",
    "I(Ankur) often try to map these words to something that I already know. This makes it easy to remember and also in the process (of mapping or creating analogy) I end up understanding the topic better. Because, if some analogy don’t work (i.e. capture the meaning) then I am forced to look for new one and so on…\n",
    "\n",
    "We call __vectorization__ the general process of turning a collection of text documents into numerical feature vectors. Documents are described by __word occurrences__ while completely __ignoring the relative position information__ of the words in the document.\n",
    "\n",
    "We want an algebraic model representing textual information as a vector, the components of this vector could represent the absence or presence (Bag of Words) of it in a document or even the importance of a term (tf–idf) in the document.\n",
    "\n",
    "__CountVectorizer__\n",
    "\n",
    "The first step in modeling the document into a vector is to create a dictionary of terms present in documents. To do that, you can simple tokenize the complete document & select all the unique terms from the document, but we know that there are some kind of words (stop words like the, are, etc) that are present in almost all documents, and what we’re doing is extracting important features from documents. So using terms like “the, at, on”, etc.. isn’t going to help us, in the differentiating them, hence we’ll just ignore them.\n",
    "\n",
    "**Implementation**\n",
    "Lets say you have two sentences in train_set as well as test_set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "766a2c4e-0e2e-4984-80c5-4acc609b9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = (\"The sky is blue.\", \"The sun is bright.\")\n",
    "test_set = (\"The sun in the sky is bright.\", \"We can see the shining sun, the bright sun.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb7f8d0f-3151-46e5-98e2-687c8ee29ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We use CountVectorizer to convert these sentences into vectors\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1252b1a-f51c-44db-9f15-00d1c766c4ec",
   "metadata": {},
   "source": [
    "The __CountVectorizer__ already uses as default analyzer called __Word__ (press Shift + TAB to see the list of arguments), which is responsible to convert the text to lowercase, accents removal, token extraction, filter stop words, etc… you can see more information by printing the class information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d8c14-d762-48a1-a1ee-9973d1d68487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "794c88cb-144f-4525-a51a-71b4f6c4fbe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 5, 'sky': 3, 'is': 2, 'blue': 0, 'sun': 4, 'bright': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(train_set)\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a068bda7-5b3c-4659-a534-785ff7bafcea",
   "metadata": {},
   "source": [
    " vocabulary_ is just a normal python dictionary. The key is token, and the value is index. We can try sorting it based on index value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb1afe27-cbee-42f7-965f-bd4166ee8e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blue': 0, 'bright': 1, 'is': 2, 'sky': 3, 'sun': 4, 'the': 5}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict = vectorizer.vocabulary_.copy()\n",
    "dict(sorted(vocab_dict.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95a6a035-e676-4e42-82ff-e4ff5f812b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('the', 5), ('sky', 3), ('is', 2), ('blue', 0), ('sun', 4), ('bright', 1)])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "21c2adfe-a386-4635-85e7-ba4e5f466794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The sun in the sky is bright.',\n",
       " 'We can see the shining sun, the bright sun.')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad859300-a09d-4531-b7d1-c571f68660f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 1, 2],\n",
       "       [0, 1, 0, 0, 2, 2]], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vec = vectorizer.transform(test_set)\n",
    "test_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "506ae26c-0c33-4c66-a349-4bb69596aa30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(['The ball is red']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1141cf3-73dd-44d3-a9ab-29f8ef3de251",
   "metadata": {},
   "source": [
    "We can also reverse the transformation operation by using inverse_transform method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a53f5ce0-44e5-41fa-910a-9a43b6d48b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['bright', 'is', 'sky', 'sun', 'the'], dtype='<U6'),\n",
       " array(['bright', 'sun', 'the'], dtype='<U6')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.inverse_transform(test_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4993fdc2-1c71-4d9c-a4b1-67c3d9200022",
   "metadata": {},
   "source": [
    "As we can see we only get the token back. Not the order of the token. That information was lost in the transformation process. This is a clear limitation of vectorization techniques, because we as humans know \"how important order is\" in natural languages.\n",
    "\n",
    "However, the main problem with this (also know as **term-frequency**) approach is that it **scales up frequent terms**, and **scales down rare terms**; which are empirically more informative than the high frequency terms. The basic intuition is that a term that occurs frequently in many documents is not a good discriminator; the important question here is: why would you, in a classification problem for instance, emphasize on a term which is present in almost all the documents in the corpus?\n",
    "\n",
    "So, technically speaking there are two problems with this approach:\n",
    "\n",
    "- document size is not taken into consideration. (normalization)\n",
    "\n",
    "- document frequency of words are also ignored. (weighting)\n",
    "\n",
    "__Tf–idf vectorizer__\n",
    "The tf-idf weight comes to solve this problem. What tf-idf gives is how important a word is to a document in a collection, and that’s why tf-idf incorporates both local and global information. Tf-idf takes in consideration not only the isolated term but also the term within the document collection.\n",
    "\n",
    "__tf-idf__ scales down the frequent terms while scaling up the rare terms; a term that occurs 10 times more than another, isn’t 10 times more important than it, that’s why tf-idf uses the logarithmic scale to do that.\n",
    "\n",
    "The use of this simple term frequency could alleviate problems like __keyword spamming__, which is when we have a repeated term in a document with the purpose of improving its ranking on an IR (Information Retrieval) system or even create a bias towards long documents, making them look more important than they are just because of the high frequency of the term in the document.\n",
    "\n",
    "Tf-idf as the name suggests, combines two different process: __term frequency__ and inverse document frequency. The term frequency of a document is generally used for normalized (__Vector Normalization__). Let’s see now, how idf (__inverse document frequency__) is then defined:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{idf}(t) = \\log\\left(\\frac{|D|}{1 + |\\{d : t \\in d\\}|}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* $|D|$ represents the total number of documents in the corpus.\n",
    "* $|\\{d : t \\in d\\}|$ represents the number of documents containing the term $t$. The '+1' in the denominator is often added to prevent division by zero for terms not present in any document and to smooth the values.\n",
    "\n",
    "The formula fot the tf-idf is then:\n",
    "$$\\text{tfidf(t)} = \\text{tf(t)} \\times\\text{idf(t)}$$\n",
    "\n",
    "and this formula has an important consequence: a high weight of the tf-idf calculation is reached when you have a high term frequency (tf) in the given document (local parameter) and a low document frequency of the term in the whole collection (global parameter).\n",
    "\n",
    "Don’t worry if things don’t make complete sense. Just rememeber, __term-frequency__ represents “the frequency of the term in the document”. And __inverse-document-frequency__ represents “in how many documents the term was present”.\n",
    "\n",
    "The first capture local information (document level) and the second captures global information (collection level). For each term, the weight is calculated by combine (here, multiplying) these two pieces of information.\n",
    "\n",
    "__Implementation__\n",
    "\n",
    "The first step is to create our training and testing document set and computing the term frequency matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "18b376e4-0bcb-480f-acf7-684d647260d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = (\"The sky is blue.\", \"The sun is bright.\")\n",
    "test_set = (\"The sun in the sky is bright.\", \"We can see the shining sun, the bright sun.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1d0866fe-67d4-44f6-8071-6d6f331650c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 5, 'sky': 3, 'is': 2, 'blue': 0, 'sun': 4, 'bright': 1}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(train_set)\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2ec297b4-d3c7-4efb-97ff-797406879fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.42519636, 0.30253071, 0.42519636, 0.42519636,\n",
       "        0.60506143],\n",
       "       [0.        , 0.37729199, 0.        , 0.        , 0.75458397,\n",
       "        0.53689271]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## transform the sentences to vectors\n",
    "vectorizer.transform(test_set).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9dda3b-141a-40b6-970f-0b83e4f9ea0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
